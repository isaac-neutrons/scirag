# Choice of LLM Service
# Options: ollama, gemini
LLM_SERVICE=ollama

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434

LLM_MODEL=gemma3:4b
EMBEDDING_MODEL=nomic-embed-text
# Embedding vector dimensions (depends on model)
# nomic-embed-text: 768, gemini-embedding-001: 3072
EMBEDDING_DIMENSIONS=768

# Gemini Configuration
GEMINI_API_KEY=you-key-here
GEMINI_MODEL=gemini-2.5-flash
# Gemini embedding model: gemini-embedding-001 (3072 dimensions)
# or text-embedding-004 (768 dimensions)

# Local MCP Server (this project's vectorstore server)
LOCAL_MCP_SERVER_URL=http://localhost:8001/sse

# MCP Servers for LLM Tool Use
# Comma-separated list of MCP server URLs that the LLM can use as tools
# Example: http://localhost:8001/sse,http://localhost:8002/sse
MCP_TOOL_SERVERS=http://localhost:8001/sse

# RavenDB Configuration
RAVENDB_URL=http://localhost:8080
RAVENDB_DATABASE=scirag

# Flask Configuration
FLASK_ENV=development
FLASK_HOST=0.0.0.0
FLASK_PORT=5000

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=DEBUG
