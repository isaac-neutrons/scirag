# Choice of LLM Service
# Options: ollama, gemini
LLM_SERVICE=ollama

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434

LLM_MODEL=gemma3:4b
EMBEDDING_MODEL=nomic-embed-text

# Gemini Configuration
GEMINI_API_KEY=you-key-here
GEMINI_MODEL=gemini-2.5-flash

# MCP Server Configuration
MCP_SERVER_URL=http://localhost:8001/mcp

# RavenDB Configuration
RAVENDB_URL=http://localhost:8080
RAVENDB_DATABASE=scirag

# Flask Configuration
FLASK_ENV=development
FLASK_HOST=0.0.0.0
FLASK_PORT=5000

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=DEBUG
